---
title: "Theme 3 Lab Notebook Volume 1"

output: html_document
---

### Description:

Date created:  20 July 2021           
Author: P. Alexander Burnham

**Summary** This document follows my progress on Theme 3 in order to provide a location for me and other members of the team to keep track of this work and maintain annotated description and functioning examples of previous iterations and related vignettes. 


### 20 July 2021:
Loaded the reticulate package and activated a new conda environment. Making sure R and Python are playing well with each other and that this format will work for keeping track of progress efficiently. By using the same conda environment that I am using in pycharm for the function's development, there should be no issues with python versions and package dependencies.

```{r setup, include=T}


# set my working directory
#setwd("~/Documents/GitHub/Theme3_LabNotebook/R_scripts")

#load the reticulate package  
library(reticulate)

# activate my previously created conda environment
use_condaenv(condaenv = "myCondaEnvironment", conda = "auto", required = FALSE)


```



First Python Chunk to make sure things are working
```{python}

# import numpy and check to ensure the version of python is correct for my conda env.
import numpy as np
import platform


print(platform.python_version())

```
Version of python is correct. Here I am writing some test python code to check that it interfaces with R in the following chunk
```{python}

# a 150 x 2 array of random numbers bound 0 to 1
randArray = np.random.rand(150,2)
print(randArray[0:3,])

```

An R chunck where I call the new python variable as py$pythonObj
```{r}
# checking out how it comes in
head(py$randArray)
# convert to data frame
randArray_R <- data.frame(py$randArray)

# load ggplot2
library(ggplot2)

# plot the python array as a scatter plot
ggplot(randArray_R, aes(x=X1, y=X2)) +
    geom_point(shape=1) + 
    geom_smooth()   

```


Plot the R dataframe in matplotlib by calling it as r.R_Obj, comes in as a dictionary
```{python, message=FALSE}

import matplotlib.pyplot as plt
from numpy.polynomial.polynomial import polyfit

# assign names x and y
x = r.randArray_R["X1"]
y =r.randArray_R["X2"]

# plot the values in the dictionary
plt.scatter(x, y)
plt.xlabel('X')
plt.ylabel('Y')
plt.show()


```



Things look good. Now pulling in the netCDF4 library in python and seeing if some code I wrote in Pycharm works well in this new environment
```{python}

import netCDF4 as nc
import numpy as np

# locate local path to .nc4 test file
fn = "/Users/pburnham/Documents/GitHub/Theme3_LabNotebook/R_scripts/data/prcp_1980.nc4"

# create netcdf data set
ds = nc.Dataset(fn)

# Pull precipitation data out of the netcd4 object
prcp=ds['prcp'][:]

# print a subset of the masked array
print(prcp[0, 500:505, 200:205])

# basic summary stats
print(np.max(prcp))
print(np.min(prcp))
print(np.mean(prcp))



```

### July 22, 2021
Here I am playing with a package called Cubes in python. It is a lightweight OLAP cubes implementation with an sql back-end. I began by working through the tutorial but found that the library is no longer being maintained and will not work with newer python and sql alchemy. I could drop down to a previous version of python but I believe I will continue on another line to avoid this for now.

```{python}

from sqlalchemy import create_engine
from cubes.tutorial.sql import create_table_from_csv
from cubes import Workspace

#begin the sqlite environment 
engine = create_engine('sqlite:///data.sqlite')

# create the table and fields
create_table_from_csv(engine,
                      "/Users/pburnham/Documents/GitHub/Theme3_LabNotebook/R_scripts/data/IBRD_Balance_Sheet__FY2010.csv",
                      table_name="ibrd_balance",
                      fields=[
                            ("category", "string"),
                            ("category_label", "string"),
                            ("subcategory", "string"),
                            ("subcategory_label", "string"),
                            ("line_item", "string"),
                            ("year", "integer"),
                            ("amount", "integer")],
                      create_id=True
                      )

# initiate the workspace
workspace = Workspace()
workspace.register_default_store("sql", url="sqlite:///data.sqlite")

# import the model as a .json file
workspace.import_model("tutorial_model.json")

# start up the browser
browser = workspace.browser("ibrd_balance")

# use the aggregate function to get an aggregation for the whole cube
result = browser.aggregate()

# print the results for record counts and amount sum
print(result.summary["record_count"])
print(result.summary["amount_sum"])

# this function call doesn't work. This is due to a lack of modernization in cubes
# this causes a problem and I don't want to work backwords to make it work
#result = browser.aggregate(drilldown=["item"])


#for record in result:
#    print(record)

```

### July 27, 2021

Here I am beginning to experiment with creating a cube from .cd4 raster files and working on the mapping aspect to get used to the file system. I load in three data sets from three different years explore their dimensions and load the data objects into a list.
```{python}
import glob
import numpy as np
import h5py
import matplotlib.pyplot as plt
import netCDF4 as nc
import csv

import os
os.environ["PROJ_LIB"] = r'C:\Users\pburnham\Anaconda3\envs\env\Library\share (location of epsg)'
#os.environ["PROJ_LIB"] = '/opt/anaconda3/pkgs/proj4-5.2.0-h6de7cb9_1006/share/proj/epsg'

from mpl_toolkits.basemap import Basemap


# set my absolute directory path for where the data are
direc ="/Users/pburnham/Documents/GitHub/Theme3_LabNotebook/R_scripts/data/dayLength/"

# create a list of all .nc4 files in my data folder that are day length data
dataPaths = glob.glob(direc+"dayl*.nc4")

# make sure paths look good
print(dataPaths)

# check out how they are read in (its a list of three paths)
print(type(dataPaths))

# in this case, time is the number of files as each .nc4 file is a year
time = len(dataPaths) # time dimension (three times: 1980-1982 inclusive)

# initialize a list to store files in during the loop
myDataList = [None] * time

# create list of netcdf data sets
for t in range(0, time):
   myDataList[t] = nc.Dataset(dataPaths[t])

# these commands print the x and y dimensions for the data sets
#lat = len(ds.dimensions['x'])
#lon = (ds.dimensions['y'])

```

Here I use matplot lib and the Basemap extension to plot a map of N. America 
```{python}
# plotting a map of north america just to make sure everything works well for visualization
####################################################################################################################

# base projection of a map
map = Basemap(projection='merc',llcrnrlon=-165.,llcrnrlat=0.,urcrnrlon=-50.,urcrnrlat=70.,resolution='i')

# add map layers/boarders
map.drawcoastlines() # draws coast lines
map.drawstates() # draws states
map.drawcountries() # draws country bourders
map.drawlsmask(land_color='Linen', ocean_color='#CCFFFF') # pick map colors
map.drawcounties() # draw county lines

# add parallels and meridians
parallels = np.arange(0,90,20.) # between 90 and 20 ever 20 degrees
meridians = np.arange(-180,180,20.) # between -180 and 180 every 20 degrees

# add them to the map
map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)
map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)

plt.show()

```


### July 28, 2021
Have gotten used to working with cd4 files and plotting maps. Here I am just showing an example of plotting day length data for January 6 1982 on a map of Hawaii. Next steps are to focus in on the cube structure. Continue to explore current pacakge options including H5PY

```{python}


# plotting some test day length data from my netcdf4 data set
####################################################################################################################

# plot the first object in my list of data sets (1982)
print(myDataList[0])

# exploring the values of these various dimensions
lat = myDataList[0].variables['lat'][:] # matrix of lats
lon = myDataList[0].variables['lon'][:]# matrix of longs
time = myDataList[0].variables['time'][:] # list of codes for 365 days
dayl = myDataList[0].variables['dayl'][:] # this is a list of matrices one for each day of the year


# the main variable in this data set is day length. the first dimensions is time in days 0-364 followed by lat and long
#print(dayl[364,:,:]) # prints last days worth of data for all recorded lats and longs


# base projection of a map around Hawaii where the data are focused here
map = Basemap(projection='merc',llcrnrlon=-165.,llcrnrlat=15.,urcrnrlon=-150.,urcrnrlat=25.,resolution='i')

# add map layers/boarders as before
map.drawcoastlines() # draws coast lines
map.drawstates() # draws states
map.drawcountries() # draws country bourders
map.drawlsmask(land_color='Linen', ocean_color='#CCFFFF') # pick map colors
map.drawcounties() # draw county lines

# add parallels and meridians, higher granularity becasue of the cropping
parallels = np.arange(0,90,5.) # # between 90 and 20 ever 5 degrees
meridians = np.arange(-180,180,5.) # between -180 and 180 every 5 degrees

# add them to the map
map.drawparallels(parallels,labels=[1,0,0,0],fontsize=10)
map.drawmeridians(meridians,labels=[0,0,0,1],fontsize=10)


#lons, lats = np.meshgrid(lonVec, latVec) Needed to expand a grid if only lat and long vecs instead of full matrix

# set coords for drawing heat map data as lat and long
# convert lat and long matrices to mapping coords
x,y = map(lon, lat)

# what is the shape of the coords
print(x.shape) # 584 by 284

# draw on the map for the 6th day of the year
dl = map.contourf(x, y, dayl[5,:,:])

# plot the color bar and add title and labels
cb = map.colorbar(dl,"right", size="5%", pad="2%")
plt.title('1982 Day Length Day 6')
cb.set_label('Day Length')

plt.show()

```














